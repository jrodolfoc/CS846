\section{Related Work}

A common solution to the imbalance problem is to replicate nodes across partitions.
Solutions for the replication problem in graph databases have been proposed in
SPAR~\cite{Pujol12}, Mondal~\cite{Mondal12} and TAO~\cite{Bronson13}. Pregel
\cite{Malewicz10}, on the other hand, relies on hash partitioning to produce an even
distribution of nodes and assumes it will provide a good probability of balanced access
between partitions. Pregel-like systems, such as Sedge~\cite{Yang12} and
Mizan~\cite{Khayyat13}, analyze workloads between supersteps and can
move/replicate nodes based on access patterns. The problem that we noted with
Pregel based systems is that they work strictly under OLAP workloads.

% TAO
TAO~\cite{Bronson13} is Facebook's geo-distributed data store. It introduces a
simple graph database API that helps developers deal with the social graph in a
more intuitive way than key-value data stores. TAO implements an eventual
consistent replication model and is heavily optimized for reads, achieving over a
billion reads per second. It achieves these goals by trying to answer all requests
straight from main memory. Facebook still uses MySQL for persistent storage, but
relies on two tiers of caching clusters to answer client requests. The two tiers are
divided by followers and leaders. Basically, clients contact followers in order to
satisfy a request. If a cache miss occurs on the follower, it contacts the leader.
The leader can contact directly the underlying MySQL cluster. It is also in charge
of maintaining consistency among followers of a geographic region, by invalidating
out-dated cached copies that followers might have. Both the leaders and the
followers implement LRU as a caching policy.

% SPAR
Pujol et al. introduce SPAR~\cite{Pujol12} as a middle-ware solution that
provides partitioning and replication to enforce local semantics. The graph
model that they exploit is one in which a node has many attributes, and those
attributes get updated. This model is in contrast to the work presented in
LinkBench \cite{Armstrong13}, where nodes are simple (i.e. have only one data
field) and updates are represented by creating new nodes with corresponding
edges. As such, SPAR is a poor candidate for storing online social network data
that follows the storage model used by Facebook, since local semantics can not
scale to that volume of neighbours. Local semantics is also excessive, since a
big volume of neighbouring data becomes cold in time.

% Mondal (SPAR with tau)
Mondal et al.~\cite{Mondal12} improve on the model introduced by SPAR, by only
requiring approximately $\tau \in [0, 1]$ of the total number of neighbours to
be local. They also exploit the read/write patterns of nodes to decide when to
replicate nodes in a different partition. In Facebook's graph, vertices are
generally read-only, so vertices see few updates. However, Mondal's model
assumes that nodes have a variable write frequency. They use this property to
cluster nodes together in replication groups. This clustering of nodes removes
the underlying graph structure, especially when all nodes have an equal write
frequency (in the case of Facebook's workload, the read-only nodes). This
results in clusters of nodes that are replicated together, but which are not
necessarily accessed together, thereby still requiring remote traversals in
order to answer queries.

% SEDGE
Yang et al. developed Sedge \cite{Yang12}, a system that focuses on graph
queries that are long lived and analytical in nature, as opposed to the online
and short lived queries of social networks. They created complimentary
partitions so that a query can start in a safe region where it is unlikely to
jump machine boundaries. Any two complimentary partitions have minimal overlap
in the edges crossing machine boundaries. Additionally, they group nodes
together using a heuristic. They record which of those groups of nodes queries
touch to create dynamic replicas of the data resulting in a safe region that
those queries can start from. They achieved 2,000 requests per second on 3-hop
neighbourhood search on a 30 million vertex, 1 billion edge graph on 31
machines. The primary difference between the system in this paper and Sedge is
that we return all data within the k-hop neighbourhood while a neighbourhood
search will return only one. However, this system was still a natural candidate
for comparison by modifying the neighbourhood search to return all vertices. We
reached out to the authors for a copy of their system but they declined.
